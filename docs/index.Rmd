---
title: "Project 3 Focus Group Participant Survey"
author: "WB"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    theme: lumen
    highlight: tango
    df_print: paged
    code_folding: hide
  pdf_document:
    toc: yes
---
<style>
.custom-box {
  border: 1px solid #b3cde0;
  background-color: #e0f3f8;
  padding: 15px;
  border-radius: 5px;
}
</style>

# Introduction 
This tutorial is an introduction to the most basic types of analysis you will undertake on survey data. It assumes a basic understanding of R, which can be achieved through many of the beginner courses you can find online like [CodeAcademy](https://www.codecademy.com/learn/learn-r&ved=2ahUKEwiDj5uJg5GJAxXWEzQIHf6JF2cQFnoECBcQAQ&usg=AOvVaw0UG42DTYemHsqsia3tBd2k) or [DataCamp](https://www.datacamp.com/courses/free-introduction-to-r&ved=2ahUKEwiDj5uJg5GJAxXWEzQIHf6JF2cQFnoECBgQAQ&usg=AOvVaw3T3T58_k6QXIeZSZKbBlDD). Once you have learned the basic structure of syntax in R and the most common functions you'll be using during an analysis project, we can begin to layer our theoretical understanding on how to undertake analysis. In particular, hopefully you can become familiar with the %>% operator as well, since this is critical for our understanding of how to structure syntax. 

There are many methods on how to approach analyses from cleaning to data presentation/visualization. Most are great but I will just present the way I was taught and how I tackle datasets. The overarching stages can be categorized into checking --> cleaning <-> analysis -> presentation. 

# Checking

## Setting up our R session

Here we set up the environment for our R session by loading all the relevant libraries (packages) used to clean, sort and analyze our data. I always start with a few key packages that I *know* I will use during my session however, this list is not exhaustive. If this is your first time loading these packages, **you will have to install them**. You should also try to update them regularly.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3)
library(haven)
library(dplyr)
library(tidyr)
library(ggplot2)
library(forcats)
library(nnet)
library(NHANES)
library(MASS)
library(naniar)
library(Hmisc)
library(mice)
library(ggeffects)
library(stringr)
library(kableExtra)
library(parallelly)
library(furrr)
library(lubridate)
library(kableExtra)
library(table1)
library(stringr)
rm(list = ls())
```

Always remember to come back to this section and add new packages that you have loaded during your exploration and analyses to ensure that when you close this R session and come back to it, your code still works! rm(list = ls()) is just a bit of code that clears your environment that I start off with when I switch between projects!

Now we set our working directory to call our file wherever we saved it. There are many different packages and options for loading in the dozens of file types you will encounter. Excel datasheets can be saved in a range of different formats (xlsx, xls, csv etc) but I often default to csv. I use the read.csv command (from the base version of R) to read in the data into a dataframe I call "df_raw". 

```{r working directory and calling our file}
setwd("/Users/will/Documents/UCSD/Project 3/Aim 1/Survey/P3Tutorial/docs")
df_raw <- read.csv("FGsurvey.csv", header = T, skip=1) %>% 
  slice(-1)
```
I'll usually open the .csv file before loading it in to give it a quick look-over and make sure it is the correct file (collaborators are notorious for sending the wrong files) and get a feel for the data structure. I have included some additional options in the above code as well that you can see to tailor our raw dataframe: header = T ensures that the first row of the .csv will be used as the column headers; skip=1 just skips reading in the first row of the dataset, which in our case was doubling up on the column names since that was included as a separate row (row 1). When looking at our .csv file, we could also see that row 2 seemed to be an artifact from exporting the qualtrics survey, which is something we do not need in our dataframe. Normally, I would have said 'skip=2' to skip both of the first two rows but it was throwing an error, and rather than spending time troubleshooting that, I can simply pipe-in a new command to drop that row from our new dataframe 'df_raw'. 

## Stop, check and think

In R Studio, in the top right corner you can see in our "Environment" tab our imported data within the new dataframe called "df_raw". **Our first job is to look at whether this new dataframe makes sense**. It says there are 29 observations of 29 variables. Frequently we will have some intuition on whether this seems reasonable for our dataset; we know from our interviews that there are a confirmed 29 respondents to this survey, so this seems reasonable. R Studio makes this easy by providing functionality to visualize dataframes/tables so we can quickly inspect our data. This can be achieved using the 'View()' command. For data that contains few respondents or observations (<1000), this is a quick and easy way to get a quick insight into our data. It is similar to viewing it within Excel.

Another check is to look at all the variable names included within the dataset. We can do this with the ls() function.
```{r varname check 1}
ls(df_raw)
```

Here it returns 29 entries, which makes sense since as we noted in the environment box top right, we have 29 variables. This is important as we can a) see the names of the variables we'll need to be calling for our analysis; b) recognize that these names are in confusing forms for quick analysis i.e., do we really want ot have to type out "What.is.your.current.position.job.title." every time we want to include the position title in a check or analysis. We can make a mental note of this and return later to clean these variable names into a more usable format. 

## Important note on survey software
Most survey software will have in-built variables they automatically collect when a response is recorded. This means that even though you didn't explicitly code these questions into a Qualtrics survey, it will still collect these data and make them available to you within your exported dataset. We can see here, for example, there are variables that we didn't code during the survey design including 'distribution channel', 'duration in seconds', 'IP Address', 'longitude and latitude'. These data can often be useful to us (IP Address is useful for checking the authenticity of responses for public facing surveys that can be spammed by bots) HOWEVER, these can also create questions of privacy for a participant. Something to be aware of. Moving on...

## Peeking at our data

We can also use the head() or tail() command, which gives a quick snippet of the first (head) or last (tail) 6 values for the called dataframe. This is useful for datasets that get into the hundreds, thousands, tens or even hundreds of thousands where viewing the whole dataset is both cumbersome and not useful. Here I called the the head and tail values for the variable Start.Date, which we can assume is the date in which they started filling out the survey. *In reality, we should never assume what any variables or values represents and should ALWAYS make sure collaborators give us their codebook.*
```{r inspect 1}
head(df_raw$Start.Date)
tail(df_raw$Start.Date)
```

This is a good second-step to checking whether the data makes sense. All these dates look reasonable for when the focus groups were held but what if we had an entry that said 2023? We'd then do a deeper dive into what happened. For now, look at the heads/tails of a few more variables and if all seems reasonable, we can move on.

Our actions at this step may vary significantly depending on our understanding of the project, field and dataset. For us, we understand that there are only supposed to be 29 respondents and the dates were reasonable, but this is because we (you) were there and can verify it. This will not always be the case with data you are cleaning so it is important to think deeply about what you are looking at. 

There are a number of reasons why weird things can occur in our dataset that would make us intervene at different stages: during exporting from survey software missing values get introduced, corrupted data during changing of file formats, even perhaps someone else made an error during the cleaning process before we recieve this data! Be aware, and always pester those who sent you data about inconsistencies. 

# Cleaning

## Value of a codebook
Cleaning a dataset is perhaps the most critical aspect of an analysis project. Doing it correctly from the get-go will save you a lot of time, effort and ensure reproducibility. I will only provide a brief introduction here as I am very much a 'learn by doing' type operator, especially since there are infinite ways to approach cleaning that will vary from person-to-person. 

Ideally, we would have access to a codebook, which allows us to fully understand what questions were asked in the dataset, and what the answers were coded as within our dataset. This allows us to accurately assign all the data to recognizable and interpretable values for our own purposes. Importantly, this also allows us to see which questions are asked as part of a 'set' so we can properly categorize and score these questions. 

## Kick-off
I almost always start by cleaning data that is common to basically all datasets: demographics. Fortunately, this survey is basically *all* demographics so let's explore some of the common demographics collected in surveys such as gender identity, age, ethnicity/race etc. I often first tabulate or summarize these data using the 'table' or dplyr functions as it gives a nice, concise visual of all the possible values for each question.

Firstly, I always create a **new** dataframe called 'df', which is where all the cleaning will take place. This allows use to a) reset our dataframe by reassigning 'df <- df_raw' at any time if we somehow make an error; b) ensure we maintain an unedited raw dataframe for reference throughout the process.
```{r Cleaning 1}
# Asign new dataframe 
df <- df_raw
# Filter out empty observations
df <- df %>% 
  dplyr::filter(Finished=="True")
```
Secondly, the 'filter' command is extremely powerful to help in both cleaning AND analysis. Here, I asked R to filter our dataset according to whether a respondent's value for 'Finished' (another automatically generated variable by Qualtrics) equaled false, indicating that they DID complete the survey. Since every respondent did complete the survey this was not necessary but in future analyses, you may have **a priori** rationale to exclude incomplete responses. This is one-of-many judgement calls you'll make as a scientist analyzing datasets. 

## Variable manipulation
```{r Cleaning 2}
# Convert date of birth variable into usable format
df$dob <- mdy(df$Please.enter.your.date.of.birth..MM.DD.YYYY..)

# Convert date of birth to age in years
df <- df %>%
    mutate(age = floor(interval(dob, Sys.Date()) / years(1)))

# Table of age group responses
head(df$age)
```
So, we did not have an age variable ready-to-go but with a few quick steps we created one. First, we had to convert our date-of-birth variable, which appeared as a character variable, into a usable format. I used a function 'mdy' from the lubridate package. You can assign dates in a variety of formats but we chose month, day, year to be consistent with American standards. 

Then we used the 'mutate' function to create a new variable called age, which was derived by creating an interval between today's date (Sys.Date()) and our generated date-of-birth variable (dob), converted into years (/ years(1)). floor() just ensures the result is rounded down to the nearest whole year. Take a quick peek at your newly created variable to make sure it looks good before moving on. 

<div class="custom-box">
Another aspect to the above code you may have noticed is that I called the package name 'dplyr' before I called the function 'filter'. This is a great habit to get in if you can as oftentimes different packages will use the same name for a function. As an example, 'filter' appears across the package dplyr **and** stats, both are fairly common packages that you may load into your analysis pipelines at some point, and this WILL create conflicts unless you specific the filter from the package you want. </div>

## Visualize
Now we can present our data:
```{r Clean and summarize}
# Median
df %>% 
  summarise(
    median_age = median(age, na.rm = TRUE),
    iqr_age = IQR(age, na.rm = TRUE)
  ) %>% 
  print()
# Boxplot
boxplot(df$age)
```
We present our age variable at its median (41 years) with interquartile range (15 years) using the summarise & print function. The boxplot function from base R also allows us to easily visualize this along with any skewness. Age is a good example of a variable that you will routinely see as skewed, which depending on the context of the data, can be totally fine or totally catastrophic! *If you receive data from a supposed randomized controlled trial and found that age is skewed, you may have some serious concerns!!!*

## Renaming
Before we proceed with any further processing, I often like to rename variables to make them more intuitive and manageable. One idiosyncrasy from importing the csv file in the way that we did was that variable names have periods in lieu of spaces (R does not allow for spaces in variable names). I prefer one word variable names so will assign these variables new names with the dplyr function 'rename'. 
```{r renaming 1}
df <- df %>% 
  dplyr::rename(gender = Which.gender.best.describes.you....Selected.Choice,
                race = Please.select.your.race.from.the.options.below....Selected.Choice,
                race_text1 = Please.select.your.race.from.the.options.below....More.than.one.race..please.specify....Text,
                race_text2 = Please.select.your.race.from.the.options.below....Other..please.specify....Text,
                ethnicity = Please.indicate.your.ethnicity.below.,
                position = What.is.your.current.position.job.title.,
                length = How.long.have.you.been.in.your.current.role.,
                consent = You.are.being.invited.to.participate.in.a.research.study.titled.Developing.and.Testing.a.Team.Communication.Training.Implementation.Strategy.for.Depression.Screening.in.a.Pediatric.Health.Care.System..This.study.is.being.led.by.Drs..Nicole.Stadnick..Ph.D...MPH.and.Lauren.Brookman.Frazee.Ph.D..from.UC.San.Diego..with.collaboration.from.Drs..Amy.Bryl.M.D..and.Cynthia.Kuelbs.M.D..from.Rady.Children.s.Hospital.San.Diego....You.were.selected.to.participate.because.of.your.role.as.a.pediatric.medical.specialty.unit.physician..nursing.provider..medical.assistant.or.behavioral.health.care.team.member..The.purpose.of.this.study.is.to.understand.the.challenges.and.supports.of.depression.care.in.pediatric.health.care.systems....Do.you.agree.to.participate..Selecting..yes..will.lead.you.to.a.few.questions.about.your.background..Selecting..no..will.take.you.to.the.end.of.this.survey.)
```
As you can see, some of the previous variables names were extremely long, riddled with periods. Different fields will have different ideas about what makes a good variable name but my preference is just to pick something that is short AND descriptive. Your process for this will depend on the particular project: some people insist that best-practice is to rename every variable in the dataset at the beginning to be consistent; I basically never do that as 1) I rarely use every variable in a dataset, 2) I should have an analysis plan before I commence data analysis that tells me what variables I am using, so would just rename the ones I will use, 3) value my time and sanity.

## Race & Ethnicity
Without going off on too much of a tangent, it is important to recognize challenges and limitations associated with the collection of race and ethnicity data. This exercise is largely concerned with how we process data post-collection but I think it is always useful to consider the design and collection phases also. So, why do we collect race and/or ethnicity data for a research project? These questions are going to vary based on the cultural context, policies and laws, and ultimately your research aims. These are both commonly included in many surveys however, there is always discussion around whether to include one over the other, both or neither. Aside from the above points, always worth remembering that many people's literacy around the difference between race and ethnicity can be quite low, which means the quality of your data may not be stellar. Perhaps we will see an example below...

First, let's look at all the responses to the first race question, which asked people to choose from a list of options:
```{r race and ethnicity 1}
race_table <- as.data.frame(table(df$race))
race_table <- race_table %>% dplyr::mutate(Perc = Freq/sum(Freq)*100) %>% 
  dplyr::arrange(desc(Freq))

race_table %>%
  kable(format = "html", caption = "Number of responses by race") %>%
  kable_paper(bootstrap_options = c("striped", "hover", "condensed", "responsive", full_width = F, position = "left"))
```
We can see that the first table looks different, more processed than ones that we used previously. This is because we are using 'kable' from the kable and kableExtra package, which allows us to produce much nicer looking tables with far more customizability. The way I often approach making kable tables is by first creating a new dataframe from a table of the variable of interest, which can be seen on the first line. This creates a dataframe that computes the counts of each response within that variable, and I have included extra code to calculate the percentage of each response using the mutate function. I have also piped in an 'arrange' command to order the counts of the responses in descending order. Then I rename the columns, using the base R function of colnames, to something descriptive - in the previous step, R automatically created a column in our dataframe called 'Freq' to describe our count data but I prefer Count. Finally, we arrive at the actual kable syntax. There are a lot of customization options here so I won't describe them all but feel free to use the ?kable documentation or see many of the online explanations. 

A few obvious things here: 

  1) White respondents make up just under half the total sample. 
  2) Two respondents preferred not to answer. 
  3) Multiple people selected either 'Other' or 'More than one race'.
  4) Two people skipped this question entirely. This is represented in our table as a row without a title.  
  
Let's start with 4...

## Missingness
Missing responses can be a serious concern for us during analysis by contributing to what is known as 'non-response' bias. Certain 'groups' (groups here is referring to any grouping variable e.g., age, gender, language spoken, number of cars owned etc) within our data may skip certain questions at higher frequencies where other 'groups' may not. Perhaps those who self-identify as black often skip this question (in this case, we can see in our data that it was 2 Medical Assistant's who identified as Hispanic ethnicity that skipped). This can cause us to make incorrect inferences during analysis, which can undermine our entire study! 

To be clear, we are not doing any statistical analysis that would be affected by missing data. In our case, we don't know why these people skipped. It may be that they also preferred not to answer but decided to just skip instead of selecting that answer (maybe they didn't read it), and in that case we could make a decision to combine those two rows. I will leave it up to you to decide whether you think this would be an appropriate way to manipulate our data. 

At the survey design stage we should do everything in our power to avoid skippable questions however, we also have to weigh the potential for people to completely drop-out of a survey if they cannot skip a question! Being unable to skip a question also means less flexibility within your survey for if things go wrong e.g., what if for whatever reason, no options applied to a respondent? what if there was a technical issue with the question? what if they don't beleive in the concept of race? These are critical things to think about during survey design and can help avoid challenges later down the track. Ultimately, if a question is skippable, was it really that critical of a data point for you to capture?

If you're interested in learning more about missingness in data, there are entire fields of statistics dedicated to this that you can engage with. I frequently use multiple imputation to account for missing data where I believe it is non-randomly missing. 

Let's move on to 3...

## Text responses
Requiring people to manually input text in your survey is always a gamble. If you spend any amount of time cleaning and analyzing survey data, you'll see some weird, wonderful, and frustrating things. Most commonly, you will simply encounter people misspelling things, which makes automating the cleaning process increasingly difficult but at least you know exactly how to treat the data (you can simply fix the spelling error). Similar with trolling: you can simply remove that datapoint if it is a silly or offensive response. Things get increasingly difficult when someone writes out a response that doesn't quite make sense. Let's take a look at the text response for our race questions.
``` {r race and ethnicity 2}
print(df$race_text1[df$race_text1 != ""])
print(df$race_text2[df$race_text2 != ""])
```
Above are the results from telling R to 'print' the responses for those variables that are not empty (represented by !=""). Here we can see what we talked about previously in that many people do not understand differences between race and ethnicity. We can see that in both text questions 1 and 2, three people give answers that we wouldn't usually consider 'races': Mexican is a nationality, not a race; Hispanic is an ethnicity, not a race; "Native" certainly could be a race but Italian definitely is not (this final entry also emphasizes another complication to text responses in that the respondent has put two answers in one question, which we'll later need to separate out). One more important aspect is that one person who selected 'Other, please specify' and another who selected 'More than one race' did not specify in-text. 

These problems arose from only a sample of 29. Imagine how messy text responses can get when you have tens of thousands of respondents! I mention this not to completely dissuade you from using text responses in a survey; sometimes text responses are extremely useful, even necessary. Text responses within surveys however, can sometimes highlight an aspect of data collection that is counterintuitive: in an attempt to force specificity from respondents, we can sometimes muddy our own data and make things less clear than if we'd provided fewer chances for specificity.

What can we do about this then?
``` {r race and ethnicity 3}
# Race variable as it currently exists. 
table(df$race)
# Let's combine clean things up. 
df <- df %>%
  mutate(
    race = na_if(race, ""),  # convert empty name to NA
    race = recode(race, 
                  "Other, please specify:" = "Other", 
                  "More than one race, please specify:" = "More than one"), 
    race = replace_na(race, "Missing")
  )
# Now let's look at our race variable. 
table(df$race)
```
I made another important decision for how we manipulate our data that could have significant effects during analysis: I chose to simply ignore the additional text responses to race and recode them simply as 'Other'. I made this decision based on a couple points: 
    1) Two of the three respondents who selected 'Other' gave text responses that I do not consider a valid option for 'race'. 
    2) One of the three respondents did not specify at all. 

We had the option of excluding these datapoints entirely however, these respondents saw the options and conciously chose 'Other', so we have to assume that there is a reason they did not select the other options, nor skip the question. Ultimately, this is a judgement call that there is often no 'perfect' answer to. We made a similar decision with 'more than one race'.

For this process, we utilized the 'recode' function from the dplyr package to adjust the values. Interestingly, you can see that we also added in the title "missing" for the missing values. Recode doesn't let us assign a value to "" (which is often how we'd tell R there is an empty value) so we use a little work around by first converting the empty cell to NA, then using the function replace_na
to reassign it the value "Missing".
``` {r race and ethnicity 4}
eth_table <- as.data.frame(table(df$ethnicity)) 

eth_table <- eth_table %>% 
  mutate(Perc = Freq/sum(Freq)*100) %>% 
  arrange(desc(Freq))

colnames(eth_table) <- c("Ethnicity", "Count", "%")

eth_table %>%
  kable("html", caption = "Number of responses by ethnicity") %>%
  kable_paper(bootstrap_options = c("striped", "hover", "condensed", "responsive", full_width = F, position = "left"))
```
Ethnicity is much more straightforward and contains no missing responses. This is shows the advantage of presenting fewer options to respondents.

What about length?
``` {r time in position}
length_table <- as.data.frame(table(df$length))

length_table$Var1 <- factor(length_table$Var1, levels = c("< 1 year", "1-3 years", "3-5 years", "5-10 years", "10+ years"), labels = c("< 1 year", "1-3 years", "3-5 years", "5-10 years", "10+ years"))

colnames(length_table) <- c("Length", "Count")

length_table %>%
  arrange(Length) %>% 
  kable("html", caption = "Length of time in current role") %>%
   kable_paper(bootstrap_options = c("striped", "hover", "condensed", "responsive", full_width = F, position = "left"))
```
This code looks different to previous lines because I converted the length_table dataframe to a factor variable. Sometimes this is necessary to ensure we can properly order the table; if we don't do this, the row orders of the table appear random. 

## Regex
'Position' needs a decent bit of clean-up due to its text-based response but since we're likely not going to report their positions verbatim, we can do a rough clean-up for now just to get the basics of regex down. Regex is a portmanteau of 'regular expression', which refers to the sequence of characters we use to match patterns within strings. This helps us mine large swatches of text data, manipulate it and then present it.
``` {r position cleaning}
df <- df %>%
  mutate(
    position_category = case_when(
      str_detect(position, regex("nurse|outpatient RN|RN", ignore_case = TRUE)) ~ "nurse",
      str_detect(position, regex("physician|fellow|Director", ignore_case = TRUE)) ~ "physician",
      str_detect(position, regex("manager|administrative", ignore_case = TRUE)) ~ "administrative",
      str_detect(position, regex("professor|prof", ignore_case = TRUE)) ~ "professor & physician",
      str_detect(position, regex("medical|CMA", ignore_case = TRUE)) ~ "medical assistant",
      TRUE ~ "other"  # Assign 'other' for any positions that don't match
    )
  )
table(df$position_category)
```
This block of code contains quite a few different parts. First, we use 'mutate', as we have previously, to create a new variable called "position_category". We then use 'case_when' (from dplyr package) and 'str_detect' (from the 'stringr' package) to tell R to assign new values (represented by ~ " ") within this new variable by matching strings within the old variable 'position'. 'regex' (I believe is a base R function) simply tells R that we are using regular expressions within the parentheses; ignore_case=TRUE is useful to tell R not to be pedantic about capitalizations, of which we have inconsistent capitalizations within our responses. Finally, the vertical '|' sign is used commonly to denote or/either i.e., we accept a match for either/or response. The ampersand sign would be used as it would in English to denote that we require BOTH strings to fully match to satisfy a match. I usually also add-in a bit of a failsafe to create an 'other' value that if certain values do not match any of our strings, it will be assigned other. Fortunately, our regex captures all the possible values and assigns them accordingly. 

For our first example, we're simply tell R to assign any strings in the old variable 'position' that match "nurse or outpatient RN or RN" to the new value of "nurse" in our new variable 'position category'. 

## Table 1
So, we've cleaned all our sociodemographics and want to present them. Here is my go-to option:
``` {r table 1}
df$length <- factor(df$length, levels = c("< 1 year", "1-3 years", "3-5 years", "5-10 years", "10+ years"))

label(df$age) <- "Age (years)"
label(df$ethnicity) <- "Ethnicity"
label(df$length) <- "Time in current position<sup>a</sup>"

footnote1 <- "<sup>a</sup> 'Time was queried categorically."

table1::table1(~ age + ethnicity + length, data=df, topclass="Rtable1-zebra", caption = "Table 1. Focus group participant demographics.", footnote = c(footnote1), render.continuous=c(.="Median [Min, Max]"))
```
The table1 function comes from the table1 package, and it is an incredibly simple and elegant function; I think of table1 similarly to how one thinks back on their first love. With this functionality, you will rarely need to fiddle around in MS Word trying to make your table 1 look nice. We can customize the row colors, names, positioning, types of summary statistics, stratify our entire table... it goes on and on. Refer to a comprehensive tutorial [here](https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html) if you are interested!

Now it is time to start using ggplot. If table1 was our first love, then ggplot is seeing the birth of your child: everything changes, and you can never see the world the same again. 

# Presentation & data visualization - Part 1
## Intro
We touched on a bit of presentation in the previous section with our table 1. I believe table 1 is the most important results you'll often present in a report or paper. This is because if your table 1 contains errors or the data appears strange then we would have far less confidence in any subsequent data that is presented. We can talk about what to look for in a table 1 in a future module if you are interested - it is a very interesting and useful skill to understand table 1s as they can often clue us into limitations, even fraudulent data, within papers. For now, it is enough to create them!

Now we will start transforming some potential data types into meaningful data visualizations. 

## Univariate figures
When we talk about univariate, we're simply talking about one response variable per observation. A good example of this is the barplot of age we produced before:
```{r univariate plot 1}
boxplot(df$age)
```

Pretty simple; pretty cool however, we have access to an even more powerful method of generating figures: ggplot. 
``` {r univariate plot 2, warning=FALSE}
df %>% 
  ggplot(aes(age, "")) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_jitter(width = 0.2, color = "darkblue", alpha = 0.4) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=1, label="Mean", geom="text", color="red") + # We also need to adjust the positioning of our geom_vline label since we have called a 'y' value and this changes the scaling.
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks=element_blank()) +
  coord_flip() # We recieve a warning message here because 2 of our respondents do not have an age value/it appears as NA.
```

With just a little bit extra code, we get something increasingly informative. More complex is not always good (the first boxplot would be totally fine as a check in your preparation or cleaning stage) however, for presentation purposes, we often want something that looks nicer or contains more information than base R boxplot() funcationality can offer us. 

## Practice plot
Let's break down ggplot() from the very basics. The first thing to appreciate about ggplot is that it uses 'layers' to build up a visualization. This is important as the ordering of layers can imapct how our figure looks in the future. We will start with calling our data. We call ggplot() and specify the dataframe we are using with 'data = df'. We then map our variables to the visuals of this figure using 'mapping=' within the aes() function, indicating to R that we want age are on our 'x' axis (i.e., it is the independent variable). 
```{r univariate plot 3}
ggplot(data = df, mapping=aes(x=age))
```

Our figure is empty because we failed to add a very important layer: geoms. These are the geometric objects used to populate our figures. There are heaps of these, and people are constantly building new packages compatabile with ggplot() to make new fancy graphs all the time. We're going to stick with geom_barplot for consistency sake. We add new layers to our figure by connecting them with the '+' sign.  
```{r univariate plot 4, warning=FALSE}
ggplot(data = df, mapping=aes(x=age)) +
  geom_boxplot()
```

Now it is starting to resemble what we're after! You can play around with other geoms if you would like (geom_histogram would also work fine with these data), and ggplot makes this simple by allowing you to change just the layer you're interested in. FYI, we often start new layers on new lines for ease of reading and editing. 

## Customizations
We also have a range of customizations **within** our geoms, like color of the borders, fill colors, sizes of points, transparency of objects etc. You can always check the help documentation to see what customizations are available for the particular geom of interest, and they can vary quite widely! Let's continue to build on our figure like previous. 
```{r univariate plot 5, warning=FALSE}
ggplot(data = df, mapping=aes(x=age)) + 
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5)
```

These are all fairly self-explanatory, and this is certainly a non-exhaustive list of things to customize except alpha, which refers to the transparency of an object. This can be useful as per above. We do not have any outliers in our data set so the 'outlier' colors and shape won't do anything but it is useful code to know. Finally, colors can also be represented by hex code if you want to get especially detailed.
``` {r univariate color brewer, warning=FALSE}
ggplot(data = df, mapping=aes(x=age)) + 
  geom_boxplot(fill = "#ea9999", color = "#e06666", outlier.color = "#85f6db", outlier.shape = 16, outlier.size = 2, alpha = 0.5)
```

These can be found on websites like [color-hex](https://www.color-hex.com/) where creators develop 'groups' of colors that work well together. There are also packages with pre-built color combos like colorbrewer. All in all, if you're anything like me you will waste hours customizing colors on figures when you could be doing actual work. 

## Shortcuts
You may have noticed that in the previous figure's code there contains a few weird differences that were not present in the original fancy figure we made. This is because we can take advantage of the fact that R will assume certain things in some situations even if we don't specify it. For example:
```{r univariate plot 6, warning=FALSE}
ggplot(data = df, mapping=aes(x=age)) +
  geom_boxplot()
```
```{r univariate plot 7}
ggplot(df, aes(age)) +
  geom_boxplot()
```

These produce the exact same plot yet the latter requires significant less code. R is able to recognize that 'df' is the dataframe we're interested in using despite not telling it using 'data='. Similar with 'aes': R knows we are mapping this function without specifically telling it. R also automatically assigns 'age' as the independent variable here. 

## Piping 
Another difference between our newer plots and our fancy original is how we assign the dataframe. Oftentimes you will see people assigning the the dataframe within the ggplot() function as we did directly above. This is not always desirable to us as it requires the data within the dateframe to be in the exact structure and format already. A useful way around this is to pipe the dataframe we want into the ggplot function.
```{r univariate piping, warning=FALSE}
df %>% 
  ggplot(., aes(age)) +
  geom_boxplot()
```

Here we first call the dataframe we want (df) and then pipe. The only change within the ggplot function is that we use '.' (period) in lieu of 'df' to tell R that we want to use the dataframe already called. This can be extremely useful as we can use a range of functions to change the dataframe before plotting.
```{r univariate piping 2, warning=FALSE}
## Filter for age
df %>% 
  dplyr::filter(age<40) %>% 
  ggplot(., aes(age)) +
  geom_boxplot()

## Filter for position
df %>% 
  dplyr::filter(position_category == "physician" | position_category == "nurse") %>% 
  ggplot(aes(age)) +
  geom_boxplot()
```

Here we used another dplyr function called 'filter' (very useful) to tell R that we're only interested in respondents aged under 40 for our first plot, or only interested in the ages of physicians OR nurses (we combined our knowledge of regex with filter here as well to get extra fancy).  

## Labelling
Labelling our figures is extremely important and is commonly done insufficiently. Ggplot gives us tremendous amounts of customizations to label our figures in any way we see fit. Let's build on our previous figure and label some useful parts. 
```{r univariate labelling, warning=FALSE}
df %>% 
  ggplot(aes(age)) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank())
```

There are two important layers for labelling: 'labs' layer, and the 'theme' layer. Labs is where we gave R all the text we're putting on are figure, and assigning them to different positions. We do not have need for a y-axis label so we assigned it an empty value with "". 

The theme layer is where we can make further adjustments to the labs layer. Here we use combinations of functions to select title, subtitle, axis title, and then adjust their typeface, size, positioning and more. We can also use 'element_blank' functions in any of these titles/text to remove these from our figures. This is useful in our case as we did not need a y-axis title so we can save some space in our figure by elimintating this (this also means that in our 'lab' layer it was unnecessary for us to assing the y-axis label with an empty value since we were elminiating the entire title in our theme layer!).

Here's another useful trick we can do:
```{r unviariate labelling 2, warning=FALSE}
df %>% 
  ggplot(aes(age)) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=0.001, label="Mean", geom="text", angle=90, color="red") +
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank())
```

We added in a dashed line with geom_vline to indicate the mean as opposed to the median already built-in to our geom_boxplot, and then used annotate to add another layer of text to our figure to label it correctly. the 'na.rm' option allows us to tell the function to ignore any 'NA's within the variable (when set to TRUE), which can often cause issues within R when calculating different things. 

This can be useful if you want to represent a difference between the median and the mean i.e., are there extreme valueds within these data and is that important to us. Effectively, we can include **any** value for the dashed line that may be useful to us. Perhaps we have a prior sample median that we want to impose on top of this one to show that our data is similar. 

## Coordinate flip
One last thing you probably noticed about the figure is that it is differently coordinated. This is often a 'taste' situation about how you prefer to read figures but I always prefer my boxplots to have the independent variable presented vertically. Let's explore a couple of ways to change the orientation of our plot.
```{r univariate coord, warning=FALSE}
# We can use the coord_flip function
df %>% 
  ggplot(aes(age)) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=0.001, label="Mean", geom="text", angle=90, color="red") +
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank()) +
  coord_flip()

# We can assign our age variable to the y-axis instead!
df %>% 
  ggplot(aes(y=age)) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=0.001, label="Mean", geom="text", angle=90, color="red") +
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks=element_blank())
```

Here we can see the usefulness of the function coord_flip as opposed to assigning 'age' to the y-axis: coord_flip simply swaps the aesthetics while keeping the scaling. Despite this, many of the prior layers we have customized now need to be updated: the x-axis is no longer the axis we want labelled and our geom_vline label is the wrong angle. 
``` {r coord flip, warning=FALSE}
df %>% 
  ggplot(aes(age)) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=0.001, label="Mean", geom="text", color="red") +
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks=element_blank()) +
  coord_flip()
```

We now have a pretty functional boxplot, not dissimilar to the earlier plot we produced. 

## Themes
Themes are a another useful customization we can add to the figures that give it a slightly more congruous feeling. 
``` {r univariate theme customs, warning=FALSE}
df %>% 
  ggplot(aes(age)) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=0.001, label="Mean", geom="text", color="red") +
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks=element_blank()) +
  coord_flip()
```

There are many 'theme' packages to apply to the overall look of our figures. theme_minimal() is a popular one but I frequently use theme_bw() also. Regardless, these can help.

## Geom_jitter
The final piece of the puzzle is another geom layer called 'geom_jitter'. This adds individual data points (much like a scatterplot) to our figure over the top of our boxplot, which is effectively a representation of our summary statistics. 
```{r geom jitter, error=TRUE}
df %>% 
  ggplot(aes(age)) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_jitter(width = 0.2, color = "darkblue", alpha = 0.4) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=0.001, label="Mean", geom="text", color="red") +
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks=element_blank()) +
  coord_flip()
```

I saved this until last because its addition to our figure requires us to amend our code a little bit. We received an error when we ran that code that says : *`geom_point()` requires the following missing aesthetics: y.*

Geom_jitter requires us to assign a value to our y-axis (in our case this appears as our x-axis due to coord_flip).
``` {r geom jitter 2, warning=FALSE}
df %>% 
  ggplot(aes(age, "")) +
  geom_boxplot(fill = "steelblue", color = "darkblue", outlier.color = "red", outlier.shape = 16, outlier.size = 2, alpha = 0.5) +
  geom_jitter(width = 0.2, color = "darkblue", alpha = 0.4) +
  geom_vline(aes(xintercept = mean(age, na.rm = TRUE)), linetype = 'dashed', color = 'red') +
  annotate(x=38.9, y=1, label="Mean", geom="text", color="red") + # We also need to adjust the positioning of our geom_vline label since we have called a 'y' value and this changes the scaling.
  labs(y = "", x = "Age (years)", title = "Distribution of age within sample", subtitle = "All positions") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"), plot.subtitle = element_text(hjust = 0.5, face="italic"), axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks=element_blank()) +
  coord_flip()
```

## Other figure types
We can apply these same lessons to other ggplot geoms very easily.
```{r geom barplot}
library(RColorBrewer)

df %>% 
  ggplot(aes(position_category, fill=position_category)) +
  geom_bar() +
  labs(y = "Count", x = "", title = "Number of personnel within each \nposition type") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"), axis.title.x=element_blank(),
        axis.text.x=element_blank(), axis.ticks=element_blank(), legend.box.background = element_rect(colour = "black"), panel.grid.minor = element_blank()) + 
  scale_fill_brewer(palette = "Pastel2") +
  guides(fill=guide_legend(title="Position")) +
  scale_y_continuous(breaks = seq(0, 7, by = 1))
```

This is a standard bar chart good for plotting frequencies/count data. Some things we added here are:
    1) We added 'fill' to our aesthetic line (aes), which can allow us to be creative with how present data. In our case, we filled each bar with the same variable as we plotted our bars with but that will not always be the case. A common use of this is when we have data across time and we want to see how another variable changes across time. 
    2) We used the ColorBrewer package to assign a color palette to our figure - I really like the Pastel palettes!
    3) We have a legend, which is automatically generated when we include a 'fill' in our aesthetic line. We manually changed its title using a 'guide' layer and put a box around it for aesthetic reasons.
    4) We manually changed the the y-axis ticks to show at every integer (the default was every 2 units but that skipped, which is nedessary to appropriately present our data!)
    
# Presentation & data visualization - Part 2
## Intro
Plotting one variable is relatively simple and gives us most of the tools we need to be able to plot more complicated figures. We have a range of tools to also plot figures where we want to plot variables against each other, whether to present them as final visualizations or to use these to probe potential associations within our data.  

## Stacked bar chart
We described this previously when creating a bar chart using geom_bar where we can assign fill a different variable to the x-assigned variable. This is a neat way to visualize changes to proportions of categorical variables compared to another variable (time is common). I use these plots in two ways, so let's visualize this using our own data:
```{r stacked bar chart}
# Count stacked bar
df %>% 
  ggplot(aes(ethnicity, fill = position_category)) +
  geom_bar(position = "stack") +
  labs(y = "Count", title = "Number of personnel within each ethnicity") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"), axis.title.x=element_blank(), axis.ticks=element_blank(), legend.box.background = element_rect(colour = "black"), panel.grid.minor = element_blank()) + 
  scale_fill_brewer(palette = "Pastel2") +
  guides(fill=guide_legend(title="Position"))

# Percentage stacked bar
df %>% 
  ggplot(aes(ethnicity, fill = position_category)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(y = "Percentage", title = "Proportion of personnel within each ethnicity") +
  theme_bw() +
  theme(plot.title = element_text(face = "bold"), axis.title.x=element_blank(), axis.ticks=element_blank(), legend.box.background = element_rect(colour = "black"), panel.grid.minor = element_blank()) + 
  scale_fill_brewer(palette = "Pastel2") +
  guides(fill=guide_legend(title="Position"))
```

First plot shows the counts of positions within each ethnicity category; second plot shows each position as a proportion of the total number of the observations within each ethnicity category. The latter is a type of normalization, which can be useful when dealing with count data with unequal numbers across categories. 

## Scatterplot
Scatterplots are a great way to display two numerical variables against each other. We have one good numerical variable, age, but we lack another good numerical variable to plot it against. We can use the function 'rnorm' from the 'stats' package in R to create a variable with fake numeric data that we can plot age against.
```{r scatterplot}
 # Let's create some fake data for a variable called distress and we can pretend it's a distress scale score. 
set.seed(5501) # Set.seed used in the context of a random number generator ensures that we produce the same set of numbers every time when we use this particular seed. This is important for reproducibility; this allows you and I to view the same data when running this code despite using a random number generator. 
df <- df %>% 
  mutate(distress = (rnorm(n=29, mean=15, sd=3))*(5*(1/age))) # Here we create the variable using RNG drawing from a normal distribution with assigned parameters however, I have also included some math at the end to ensure that there is some association between the two variables. 

df %>% 
  ggplot(., aes(x = age, y = distress)) +
  geom_point(aes(color="red", alpha = 0.8)) + 
  geom_smooth(method = "lm", color = "blue", se = TRUE, linetype = "dashed", size=0.5) +
  labs(title = "Relationship between Age and Distress", x = "Age (years)", y = "Distress score") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size=14), legend.position="none")
```

Here we can see what appears to be a negative association between age and distress i.e., younger people had higher distress scores compared to older people. We have included a trend line using the geom 'geom_smooth' with linear regression as the method to generate the parameters of this line, including the standard error. We can go one step further if we want additional stratification within our plot.
```{r scatterplot 2, warning=FALSE}
df %>% 
  ggplot(aes(x = age, y = distress)) +
  geom_point(aes(color = position_category)) + 
  geom_smooth(method = "lm", color = "blue", se = TRUE, linetype = "dashed", size = 0.5, alpha=0.2) +
  labs(title = "Relationship between Age and Distress", x = "Age (years)", y = "Distress score") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold", size = 14)) +
  scale_color_brewer(palette = "Set1") +
  guides(color = guide_legend(title = "Position"), alpha = "none")
```

Now we have assigned colors for each data point to represent different positions. So now we not only have information on the association between age and distress, but have also have some information between distress and position (medical assistants appear to have higher distress compared to professor/physicians) *AND* position and age (younger people tend to be medical assistants). All of this is very intuitive when we think about it, and understanding these types of relationships is derived from a crucial discipline called 'causal inference'. This is not a statistics course so we will not venture further nevertheless, causal inference is something that underpins many fields who employ statistics to determine relationships between anything!